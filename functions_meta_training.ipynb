{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import odeint \n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pylab as py\n",
    "from math import *\n",
    "import numpy as np\n",
    "from scipy.optimize import newton\n",
    "from matplotlib import animation, rc\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IPython.display import HTML\n",
    "\n",
    "import warnings\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the movement of the double pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differential equations describing the system\n",
    "def double_pendulum(u,t,m1,m2,L1,L2,g):\n",
    "    # du = derivatives\n",
    "    # u = variables\n",
    "    # p = parameters\n",
    "    # t = time variable\n",
    "\n",
    "    du = np.zeros(4)\n",
    "    c = np.cos(u[0]-u[2])  # intermediate variables\n",
    "    s = np.sin(u[0]-u[2])  # intermediate variables\n",
    "\n",
    "    du[0] = u[1]   # d(theta 1)\n",
    "    du[1] = ( m2*g*np.sin(u[2])*c - m2*s*(L1*c*u[1]**2 + L2*u[3]**2) - (m1+m2)*g*np.sin(u[0]) ) /( L1 *(m1+m2*s**2) )\n",
    "    du[2] = u[3]   # d(theta 2)\n",
    "    du[3] = ((m1+m2)*(L1*u[1]**2*s - g*np.sin(u[2]) + g*np.sin(u[0])*c) + m2*L2*u[3]**2*s*c) / (L2 * (m1 + m2*s**2))\n",
    "\n",
    "    return du"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the Training Data into training batches and scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = input_data[i:i+tw]\n",
    "        train_label = input_data[i+tw:i+tw+1]\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return inout_seq\n",
    "\n",
    "def preprocess_sol(train_data_normalized, train_window):\n",
    "    train_inout_seq= create_inout_sequences(train_data_normalized.reshape(len(train_data_normalized), 9), train_window)\n",
    "    return train_inout_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the path of the double pendulum and saving the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makepath_param(theta1, theta2, Nt, tfinal, m1,m2,l1,l2, g = 9.81):\n",
    "    u0 = [np.pi*theta1/180, 0, np.pi*theta2/180, 0]\n",
    "\n",
    "    #timesteps\n",
    "    dt = tfinal / Nt\n",
    "    \n",
    "    t = np.linspace(0, tfinal, Nt)\n",
    "    sol = odeint(double_pendulum, u0, t, args=(m1,m2,l1,l2,g))\n",
    "    \n",
    "    # create a parameter list\n",
    "    params = np.array([m1, m2, l1, l2, dt])\n",
    "    \n",
    "    # giving the correct shape\n",
    "    params_repeated = np.tile(params, (sol.shape[0], 1))\n",
    "    \n",
    "    # stack them on the simulation data\n",
    "    sol_with_params = np.hstack((sol, params_repeated))\n",
    "    \n",
    "    return sol_with_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slop for multiple simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_simulations(theta1, theta2, change_of_angle, amount_of_different_inits, Nt, tfinal, m1_params, m2_params, l1_params, l2_params, g):\n",
    "    simulation_sets = []\n",
    "    print(f'Amount of chosen timesteps is {Nt}')\n",
    "    print(f'Total simulation time is {tfinal} s')\n",
    "    #This slope is defined for different initial condition\n",
    "    for i in range(0, amount_of_different_inits):\n",
    "        theta1 += change_of_angle\n",
    "        theta2 -= change_of_angle\n",
    "        print(f\"theta 1 ist {theta1}\")\n",
    "        print(f\"theta 2 ist {theta2}\")\n",
    "        for m1 in m1_params:\n",
    "            for m2 in m2_params:\n",
    "                for l1 in l1_params:\n",
    "                    for l2 in l2_params:\n",
    "                        sol = makepath_param(theta1, theta2, Nt, tfinal, m1,m2,l1,l2,g)\n",
    "                        print(f\"m1 = {m1}; l1 = {l1}; m2 = {m2}; l2 = {l2}\\n\")\n",
    "                        simulation_sets.append(sol)\n",
    "    return simulation_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the Simulation Data into a Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_splitter_and_scaler(scaler, Simulation_datasets, test_size = 0.2):\n",
    "    train_splits = []\n",
    "    test_splits = []\n",
    "\n",
    "    for dataset in Simulation_datasets:\n",
    "        split_index = int(len(dataset) * (1 - test_size))\n",
    "        train_split = dataset[:split_index]\n",
    "        test_split = dataset[split_index:]\n",
    "        train_splits.append(train_split)\n",
    "        test_splits.append(test_split)\n",
    "    \n",
    "    for i in range(len(train_splits)):\n",
    "        dataset = train_splits[i]\n",
    "        train_window = 10\n",
    "        \n",
    "        #separating the params since those valuus shouldnt be scale\n",
    "        params_and_dt = dataset[:, 4:]\n",
    "        m1, m2, l1, l2, dt = params_and_dt[0]\n",
    "        print(f\"m1 = {m1}; l1 = {l1}; m2 = {m2}; l2 = {l2}; dt = {dt}\\n\")\n",
    "        \n",
    "        train_data_normalized = scaler.fit_transform(dataset[:, :4].reshape(-1, 4))\n",
    "        train_data_normalized_with_params = np.concatenate((train_data_normalized, params_and_dt), axis=1)    \n",
    "        train_data_normalized_with_params = torch.FloatTensor(train_data_normalized_with_params).reshape(len(dataset), 9)\n",
    "\n",
    "        #A change of the reshape function inside the preprocess_sol might be required in case you change the amount of timesteps\n",
    "        train_inout_seq = preprocess_sol(train_data_normalized_with_params, train_window)\n",
    "        \n",
    "        train_splits[i] = train_inout_seq\n",
    "        \n",
    "    return train_splits, test_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to test the model during the Trainingsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_tester(model, data, num_layers, device, hidden_layer_size = 100):\n",
    "    \n",
    "    train_window = 10\n",
    "    highest = len(data)- train_window\n",
    "    \n",
    "    call_begin_position = np.random.randint(len(data) - train_window - 1)\n",
    "    call_end_position = call_begin_position + train_window\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    \n",
    "    data_normalized = scaler.fit_transform(data.reshape(-1, 4))\n",
    "    \n",
    "    test_batch = data_normalized[call_begin_position:call_end_position].reshape(train_window,4).tolist()\n",
    "    test_batch = torch.tensor(test_batch).float()\n",
    "    \n",
    "    model.eval().cpu()\n",
    "    \n",
    "    model.c_h = (torch.zeros(num_layers, 1, hidden_layer_size).to(device),\n",
    "                        torch.zeros(num_layers, 1, hidden_layer_size).to(device))\n",
    "    prediction = model(test_batch).to(device)\n",
    "    prediction = prediction.cpu().detach().numpy()\n",
    "    \n",
    "    test_comp_pos = call_end_position + 1\n",
    "    test_data = data_normalized[test_comp_pos].reshape(1,4).tolist()\n",
    "    \n",
    "    test_error = np.abs(test_data - prediction)\n",
    "    test_error = np.mean(test_error)\n",
    "    \n",
    "    return prediction, test_data, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def physics_loss_1d(predicted_T, current_T, dx, dt, C):\n",
    "\n",
    "    # Compute the 1D Laplacian (finite difference approximation)\n",
    "    # laplacian = (current_T[2:] - 2*current_T[1:-1] + current_T[:-2]) / dx**2\n",
    "    laplace = []\n",
    "    xx_T = current_T*0\n",
    "    for i in range(1, len(current_T)-1):\n",
    "        xx_T[i] = (current_T[i+1] - 2*current_T[i] + current_T[i-1]) / dx**2\n",
    "        laplace.append(xx_T)\n",
    "\n",
    "    laplace_arr = np.array(laplace)\n",
    "\n",
    "    # Compute the time derivative\n",
    "    # time_derivative = (predicted_T[1:-1] - current_T[1:-1]) / dt\n",
    "    time_derivative = (predicted_T - current_T) / dt\n",
    "    # print(f\"This is the ffirst nod {time_derivative[0]} \")\n",
    "    # print(f\"This is the derivative of the last nod {time_derivative[-1]}\")\n",
    "    loss_list = []\n",
    "\n",
    "    for i in range(0, len(laplace[0])-2):\n",
    "        physics_loss = time_derivative[i+1] - C * laplace[i]\n",
    "        loss_list.append(physics_loss)\n",
    "\n",
    "    return physics_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conservation of Energy equation, as physical information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_calculation_np(theta1, theta1_dot, theta2, theta2_dot, mass1, mass2, length1, length2, g=9.81):\n",
    "\n",
    "    kinetic_energy = 0.5 * mass1 * length1**2 * theta1_dot**2 + 0.5 * mass2 * ((length1 * theta1_dot)**2 + (length2 * theta2_dot)**2 + 2 * length1 * length2 * theta1_dot * theta2_dot * np.cos(theta1 - theta2))\n",
    "\n",
    "    potential_energy = -mass1 * g * length1 * np.cos(theta1) - mass2 * g * (length1 * np.cos(theta1) + length2 * np.cos(theta2))\n",
    "\n",
    "    total_energy = abs(kinetic_energy + potential_energy)\n",
    "\n",
    "    return total_energy\n",
    "\n",
    "def conservation_of_energy(predicted_state, previous_state, target_state, mass1, mass2, length1, length2, g=9.81):\n",
    "\n",
    "    #extracting the data from the arrays\n",
    "    theta1, theta1_dot, theta2, theta2_dot = predicted_state[0, :]\n",
    "    prev_theta1, prev_theta1_dot, prev_theta2, prev_theta2_dot = previous_state[0, :]\n",
    "    target_theta1, target_theta1_dot, target_theta2, target_theta2_dot = target_state[0, :]\n",
    "\n",
    "    #Calculating all the type specific energies\n",
    "    predicted_energy = energy_calculation_np(theta1, theta1_dot, theta2, theta2_dot, mass1, mass2, length1, length2, g=9.81)\n",
    "    previous_energy = energy_calculation_np(prev_theta1, prev_theta1_dot, prev_theta2, prev_theta2_dot, mass1, mass2, length1, length2, g=9.81)\n",
    "    target_energy = energy_calculation_np(target_theta1, target_theta1_dot, target_theta2, target_theta2_dot, mass1, mass2, length1, length2, g=9.81)\n",
    "\n",
    "    #calculating the difference = loss of the energies\n",
    "    energy_loss = predicted_energy - previous_energy\n",
    "\n",
    "    return energy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lagrange Equation as physical information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differential_operator(current_pos, previous_pos, timestep):\n",
    "    slope = (previous_pos - current_pos) / timestep\n",
    "    return slope\n",
    "\n",
    "def lagrange_loss_single_value(predicted_state, previous_state, target_state, timestep, m1, m2, l1, l2, g=9.81):\n",
    "    #extracting the data from the arrays\n",
    "    theta1, theta1_dot, theta2, theta2_dot = predicted_state[0, :]\n",
    "    prev_theta1, prev_theta1_dot, prev_theta2, prev_theta2_dot = previous_state[0, :]\n",
    "    target_theta1, target_theta1_dot, target_theta2, target_theta2_dot = target_state[0, :]\n",
    "        \n",
    "    theta1_ddot = differential_operator(theta1_dot, prev_theta1_dot, timestep)\n",
    "    theta2_ddot = differential_operator(theta2_dot, prev_theta2_dot, timestep)\n",
    "        \n",
    "    lagrange_eq_theta1 = (\n",
    "                            (m1 + m2) * theta1_ddot * l1**2\n",
    "                            + m2 * l1**2 * theta2_ddot * l2**2 * np.cos(theta1 - theta2)\n",
    "                            + m2 * l1**2 * theta2_dot**2 * l2**2 * np.sin(theta1 - theta2)\n",
    "                            + (m1 + m2) * g * l1 * np.sin(theta1)\n",
    "                        )\n",
    "    lagrange_eq_theta2 = (\n",
    "                            theta2_ddot * l2\n",
    "                            + theta1_ddot * l1**2 * l2 * np.cos(theta1 - theta2)\n",
    "                            - theta1_ddot * l1**2 * l2 * np.sin(theta1 - theta2)\n",
    "                            + g * np.sin(theta2)\n",
    "                        )\n",
    "        \n",
    "    lagrange_loss = np.abs(lagrange_eq_theta1) + np.abs(lagrange_eq_theta2)\n",
    "    return lagrange_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the full OneStep function.   it\n",
    "def OneStep(model, model_layers, data, filename, filepath_excel, test_nr, steps = 100, g = 9.81):\n",
    "    print('data set length =', len(data))\n",
    "    train_window = 10\n",
    "    initial_predict_data = data[:, :4]\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    train_data_normalized = scaler.fit_transform(initial_predict_data.reshape(-1, 4))\n",
    "    fut_pred = len(data) -train_window\n",
    "    test_inputs = train_data_normalized[0:train_window].reshape(train_window,4).tolist()\n",
    "    model.eval().cpu()\n",
    "    preds = test_inputs.copy()\n",
    "    t2 = test_inputs\n",
    "    hidden_layer_size = 100\n",
    "    x = 0\n",
    "    for i in range(fut_pred):\n",
    "        seq = torch.FloatTensor(t2[i:]).cpu()\n",
    "        model.c_h = (torch.zeros(model_layers, 1, hidden_layer_size),\n",
    "                        torch.zeros(model_layers, 1, hidden_layer_size))\n",
    "        x = model(seq).cpu()\n",
    "        x = x.cpu()\n",
    "        preds.append(x.detach().numpy())\n",
    "        t2.append(x.detach().numpy())\n",
    "    actual_predictions = scaler.inverse_transform(np.array(preds ).reshape(-1,4))\n",
    "    print(len(actual_predictions))\n",
    "\n",
    "    # the following will plot the lower mass path for steps using the actual ODE sover\n",
    "    # and the predicitons\n",
    "    plt.figure( figsize=(10,5))\n",
    "    u0 = data[:,0]     # theta_1\n",
    "    u1 = data[:,1]     # omega 1\n",
    "    u2 = data[:,2]     # theta_2\n",
    "    u3 = data[:,3]     # omega_2\n",
    "    up0 = actual_predictions[:,0]     # theta_1\n",
    "    up1 = actual_predictions[:,1]     # omega 1\n",
    "    up2 = actual_predictions[:,2]     # theta_2\n",
    "    up3 = actual_predictions[:,3]     # omega_2\n",
    "    m1 = data[:, 4][0]\n",
    "    m2 = data[:, 5][0]\n",
    "    l1 = data[:, 6][0]\n",
    "    l2 = data[:, 7][0]\n",
    "    x1 = l1*np.sin(u0);          # First Pendulum\n",
    "    y1 = -l1*np.cos(u0);\n",
    "    x2 = x1 + l2*np.sin(u2);     # Second Pendulum\n",
    "    y2 = y1 - l2*np.cos(u2);\n",
    "    xp1 = l1*np.sin(up0);          # First Pendulum\n",
    "    yp1 = -l1*np.cos(up0);\n",
    "    xp2 = xp1 + l2*np.sin(up2);     # Second Pendulum\n",
    "    yp2 = yp1 - l2*np.cos(up2); \n",
    "    print(x2[0], y2[0])\n",
    "    plt.plot(x2[0:steps], y2[0:steps], color='r')\n",
    "    plt.plot(xp2[0:steps],yp2[0:steps] , color='g')\n",
    "\n",
    "    ###############################################################################\n",
    "    dt = data[:, 8][0]\n",
    "    Nt = len(data[:, 8])\n",
    "    tfinal = dt * Nt\n",
    "    t = np.linspace(0, tfinal, Nt)\n",
    "\n",
    "    energy_data = energy_calculation_np(u0, u1, u2, u3, m1, m2, l1, l2, g)\n",
    "    energy_predicted = energy_calculation_np(up0, up1, up2, up3, m1, m2, l1, l2, g)\n",
    "\n",
    "    ###############################################################################\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ## Only necessary when addapting the lookback\n",
    "    # lookback += 200\n",
    "    plt.suptitle('Simulation vs Prediction - ' + filename, fontsize=16)\n",
    "\n",
    "    lookback = 0\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.plot(t[lookback:], data[lookback:, 0], label='Simulation')\n",
    "    plt.plot(t[lookback:], actual_predictions[:, 0], label='Prediction')\n",
    "    plt.ylabel('Theta 1')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.plot(t[lookback:], data[lookback:, 1], label='Simulation')\n",
    "    plt.plot(t[lookback:], actual_predictions[:, 1], label='Prediction')\n",
    "    plt.ylabel('Omega 1')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.plot(t[lookback:], data[lookback:, 2], label='Simulation')\n",
    "    plt.plot(t[lookback:], actual_predictions[:, 2], label='Prediction')\n",
    "    plt.ylabel('Theta 2')\n",
    "    plt.xlabel('Time in s')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.plot(t[lookback:], data[lookback:, 3], label='Simulation')\n",
    "    plt.plot(t[lookback:], actual_predictions[:, 3], label='Prediction')\n",
    "    plt.ylabel('Omega 2')\n",
    "    plt.xlabel('Time in s')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.plot(t[lookback:], energy_data, label='Simulation')\n",
    "    plt.plot(t[lookback:], energy_predicted, label='Prediction')\n",
    "    plt.ylabel('Omega 2')\n",
    "    plt.xlabel('Time in s')\n",
    "    plt.legend()\n",
    "\n",
    "    ##############################################################################\n",
    "    \n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame({ 'time': t,\n",
    "        f'theta1_actual': data[:, 0], 'omega1_actual': data[:, 1],\n",
    "        'theta2_actual': data[:, 2], 'omega2_actual': data[:, 3],\n",
    "        'energy_actual': energy_data,\n",
    "        'm1': data[:, 4], 'm2': data[:, 5], 'l1': data[:, 6], 'l2': data[:, 7], 'dt': data[:, 8],\n",
    "        'theta1_predicted': actual_predictions[:, 0], 'omega1_predicted': actual_predictions[:, 1],\n",
    "        'theta2_predicted': actual_predictions[:, 2], 'omega2_predicted': actual_predictions[:, 3],\n",
    "        'energy_predicted': energy_predicted,\n",
    "    })\n",
    "\n",
    "    filename = f'Test_nr_{test_nr}_{filename}'\n",
    "    #Combination filepath and filename\n",
    "    full_path = os.path.join(filepath_excel, filename)\n",
    "\n",
    "    # safe csv\n",
    "    df.to_csv(f'{full_path}.csv', index=False)\n",
    "    \n",
    "    return actual_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_trainer(model_freeze, scaler, train_datasets, test_datasets, epochs, subcycle, training_type, data_handling, freeze, filename, filepath_excel, device, num_layers, hidden_layer_size):    \n",
    "    loss_function = nn.MSELoss()\n",
    "    loss_function2 = nn.L1Loss()\n",
    "    \n",
    "    #Choosing the Optimizer\n",
    "    optimizer_freeze = torch.optim.Adam(model_freeze.parameters(), lr=0.0005)\n",
    "    \n",
    "    # necessary for the documentation of the Trainingevolution\n",
    "    total_loss_list = []\n",
    "    physical_loss_list = []\n",
    "    red_physical_loss_list = []\n",
    "    data_loss_list = []\n",
    "    test_error_mean_list = []\n",
    "    test_error_MAE_list = []\n",
    "    test_error_MSE_list = []\n",
    "    trainingsequences = []\n",
    "    counter = 0\n",
    "\n",
    "    for j in range(0, len(train_datasets)):      \n",
    "        if freeze == True:\n",
    "            num_layers_to_freeze = j\n",
    "            model_freeze.freeze_layers(model_freeze, j)\n",
    "            if num_layers_to_freeze == 0:\n",
    "                print(f'First training cycle is conducted without freezed layers: {num_layers_to_freeze}')\n",
    "            else:\n",
    "                print(f'Amount of freezed layers: {num_layers_to_freeze}')\n",
    "\n",
    "        print(f\"Cycle {j+1} of {len(train_datasets)} cycles has started\")\n",
    "        \n",
    "        if data_handling == \"structured\":\n",
    "            train_inout_seq = train_datasets[j]\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            counter += 1\n",
    "            for _ in range(subcycle):\n",
    "                \n",
    "                if data_handling == \"random\":\n",
    "                    #random call off the trainingdataset\n",
    "                    random_call = np.random.randint(len(train_datasets))\n",
    "                    train_inout_seq = train_datasets[random_call]\n",
    "                \n",
    "                #random call of the training batch\n",
    "                num = np.random.randint(len(train_inout_seq))\n",
    "                X_train = train_inout_seq[num][0][:,:4].to(device)\n",
    "                y_target = train_inout_seq[num][1][:,:4].to(device)\n",
    "                \n",
    "                #get the last 4 pendulum-parameter\n",
    "                params = train_inout_seq[num][0][:,4:].to(device)\n",
    "                m1, m2, L1, L2, timestep = params[0]\n",
    "                \n",
    "                optimizer_freeze.zero_grad()\n",
    "\n",
    "                model_freeze.c_h = (torch.zeros(len(train_datasets), 1, hidden_layer_size).to(device),\n",
    "                                torch.zeros(len(train_datasets), 1, hidden_layer_size).to(device))\n",
    "\n",
    "                y_pred_freeze = model_freeze(X_train)\n",
    "                \n",
    "                single_loss = loss_function(y_pred_freeze.view(1,4), y_target)\n",
    "                \n",
    "                #preparing the data for the physical loss\n",
    "                #getting the previous timestep\n",
    "                X_train_3D = torch.tensor(X_train, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "                prev_val = X_train_3D[0, -1, :]\n",
    "                prev_val = prev_val.detach().cpu().numpy()\n",
    "                prev_val_real = scaler.inverse_transform(prev_val.reshape(1, -1))\n",
    "\n",
    "                #rescaling the actual prediction\n",
    "                prediction_real = y_pred_freeze.detach().cpu().numpy()\n",
    "                prediction_real = scaler.inverse_transform(prediction_real.reshape(1, -1))\n",
    "\n",
    "                #for comparison the data target\n",
    "                y_target_real = y_target.detach().cpu().numpy()\n",
    "                y_target_real = scaler.inverse_transform(y_target_real.reshape(1, -1))\n",
    "\n",
    "                if training_type == \"Data_driven\":\n",
    "                    total_loss = single_loss\n",
    "                elif training_type == \"Energy\":\n",
    "                    energy_loss = conservation_of_energy(prediction_real, prev_val_real, y_target_real, m1, m2, L1, L2, g=9.81)\n",
    "                    energy_loss = abs(energy_loss)\n",
    "                    alpha = 0.1 + 0.9 * (i / epochs)\n",
    "                    total_loss = single_loss + alpha * energy_loss\n",
    "                elif training_type == \"Lagrange\":\n",
    "                    lagrange_loss = lagrange_loss_single_value(prediction_real, prev_val_real, y_target_real, timestep, m1, m2, L1, L2)\n",
    "                    alpha = 0.1 + 0.9 * (i / epochs)\n",
    "                    total_loss = single_loss + alpha * lagrange_loss\n",
    "                else:\n",
    "                    raise ValueError(\"Ungültiger training_type. Er muss 'Lagrange', 'Energy' oder 'Datadriven' sein.\")\n",
    "                    \n",
    "                total_loss.backward()\n",
    "                optimizer_freeze.step()\n",
    "\n",
    "                test_dataset_tensor = torch.tensor(test_datasets[j][:,:4])\n",
    "                prediction, test_data, test_error_mean = training_tester(model_freeze, test_dataset_tensor, num_layers, device, hidden_layer_size)\n",
    "\n",
    "                test_loss_MSE = loss_function(torch.tensor(test_data), torch.tensor(prediction))\n",
    "                test_loss_MAE = loss_function2(torch.tensor(test_data), torch.tensor(prediction))\n",
    "\n",
    "            if training_type == \"Data_driven\":\n",
    "                not_considered = 0\n",
    "                physical_loss_list.append(not_considered)\n",
    "            elif training_type == \"Energy\":\n",
    "                physical_loss_list.append(energy_loss.item())\n",
    "            elif training_type == \"Lagrange\":\n",
    "                physical_loss_list.append(lagrange_loss.item())    \n",
    "            else:\n",
    "                raise ValueError(\"Ungültiger training_type. Er muss 'Lagrange', 'Energy' oder 'Datadriven' sein.\")      \n",
    "                       \n",
    "            total_loss_list.append(total_loss.item())\n",
    "            data_loss_list.append(single_loss.item())\n",
    "            test_error_mean_list.append(test_error_mean.item())\n",
    "            test_error_MAE_list.append(test_loss_MAE.item())\n",
    "            test_error_MSE_list.append(test_loss_MSE.item())\n",
    "\n",
    "            trainingsequences.append(counter)\n",
    "\n",
    "            if i%25 == 1:\n",
    "\n",
    "                print(f'epoch: {i:3} data loss of freezed model: {single_loss.item():10.8f}')\n",
    "                print(f'epoch: {i:3} total loss of freezed model: {total_loss.item():10.8f}')\n",
    "                if training_type == \"Energy\":\n",
    "                    print(f'epoch: {i:3} Energy loss of freezed model: {energy_loss.item():10.8f}')\n",
    "                if training_type == \"Lagrange\":\n",
    "                    print(f'epoch: {i:3} Lagrange loss of freezed model: {lagrange_loss.item():10.8f}')\n",
    "                print(f'the test error ist {test_error_mean.item()}')\n",
    "                # print(f'the test error ist {test_loss.item()}')\n",
    "\n",
    "        df_losses = pd.DataFrame({\n",
    "            \"Epoch\": trainingsequences,\n",
    "            \"Total Loss\": total_loss_list,\n",
    "            \"Physical Loss\": physical_loss_list,\n",
    "            \"Data Loss\": data_loss_list,\n",
    "            \"Test Error Mean\": test_error_mean_list,\n",
    "            \"Test Error Loss Function\": test_error_MAE_list,\n",
    "            \"Test Error Loss Function MSE\": test_error_MSE_list\n",
    "        })\n",
    "\n",
    "        # Save csv\n",
    "        df_losses.to_csv(os.path.join(filepath_excel, f\"trainingloss_cycle{j+1}_{filename}.csv\"), index=False)\n",
    "        \n",
    "        print(f\"CSV-Datei gespeichert als 'trainingloss_cycle{j+1}_{filename}.csv'\")\n",
    "\n",
    "        # Create plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        plt.plot(df_losses[\"Epoch\"], df_losses[\"Total Loss\"], label=\"Total Loss\", color=\"blue\")      \n",
    "        plt.plot(df_losses[\"Epoch\"], df_losses[\"Physical Loss\"], label=\"Physical Loss\", color=\"red\")\n",
    "        plt.plot(df_losses[\"Epoch\"], df_losses[\"Data Loss\"], label=\"Data Loss\", color=\"orange\")\n",
    "        plt.plot(df_losses[\"Epoch\"], df_losses[\"Test Error Mean\"], label=\"Test Error Mean\", color=\"green\")  \n",
    "        plt.plot(df_losses[\"Epoch\"], df_losses[\"Test Error Loss Function\"], label=\"Test Error Loss Function\", color=\"purple\") \n",
    "        plt.plot(df_losses[\"Epoch\"], df_losses[\"Test Error Loss Function MSE\"], label=\"Test Error Loss Function MSE\", color=\"black\")  \n",
    "\n",
    "        #titles\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training Loss über die Epochen\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Period {j+1} was completed and the model states have been saved at this moment\")\n",
    "        torch.save(model_freeze.state_dict(), filename)\n",
    "    print(\"training finished\")\n",
    "    \n",
    "    torch.save(model_freeze.state_dict(), filename)\n",
    "    \n",
    "    return model_freeze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
